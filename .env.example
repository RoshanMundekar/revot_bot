# ========================================
# LLM Provider Configuration
# ========================================

# Choose your LLM provider: openai, groq, or watsonx
LLM_PROVIDER=groq

# ========================================
# OpenAI Configuration
# ========================================
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-3.5-turbo

# ========================================
# Groq Configuration
# ========================================
GROQ_API_KEY=your_groq_api_key_here
GROQ_MODEL=llama3-70b-8192
# Available models: llama3-70b-8192, llama3-8b-8192, mixtral-8x7b-32768, gemma-7b-it

# ========================================
# WatsonX Configuration
# ========================================
WATSONX_API_KEY=your_watsonx_api_key_here
WATSONX_PROJECT_ID=your_watsonx_project_id_here
WATSONX_MODEL=ibm/granite-13b-chat-v2
WATSONX_URL=https://us-south.ml.cloud.ibm.com
# Available models: ibm/granite-13b-chat-v2, meta-llama/llama-3-70b-instruct, etc.

# ========================================
# General LLM Parameters
# ========================================
MAX_TOKENS=500
TEMPERATURE=0.7

# Server Configuration
PORT=8000

# CORS Configuration (comma-separated list of allowed origins)
# Use * for development, specific domains for production
CORS_ORIGINS=*
# Example for production:
# CORS_ORIGINS=https://yourdomain.com,https://www.yourdomain.com
